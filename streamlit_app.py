import streamlit as st
from openai import OpenAI

# Show title and description.
st.title("ğŸ’¬ loplat chatbot")
st.write(
    "ì‹ ì„¸ê³„ë°±í™”ì  ê°•ë‚¨ì ì—ì„œ ê¶ê¸ˆí•œ ê²ƒì€ ë¬´ì—‡ì´ë“  ë¬¼ì–´ë³´ì„¸ìš”."
    "You can also learn how to build this app step by step by [following our tutorial](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps)."
)

# Ask user for their OpenAI API key via `st.text_input`.
# Alternatively, you can store the API key in `./.streamlit/secrets.toml` and access it
# via `st.secrets`, see https://docs.streamlit.io/develop/concepts/connections/secrets-management
# openai_api_key = st.text_input("OpenAI API Key", type="password")
# OPENAI_API_KEY = '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'
# openai_api_key = OPENAI_API_KEY #st.secrets["OPENAI_API_KEY"]

from datetime import datetime
import os
OPENAI_API_KEY = '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'
os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY

from langchain_openai import OpenAIEmbeddings, ChatOpenAI
embeddings = OpenAIEmbeddings()
llm = ChatOpenAI(
    model_name='gpt-4o-mini',
    temperature=0,
    max_tokens=1024
)

PINECONE_KEY = 'pcsk_5LSZ4P_ANxevsPE5nS6idLg9CZPjMzPVNcj1JUHX6atoMxHq4zJjHZ6nBcW4PVbkgDZfa6'
from pinecone import Pinecone, ServerlessSpec
pc = Pinecone(api_key=PINECONE_KEY)
pc_i = pc.Index("shingangsro")

from sentence_transformers import SentenceTransformer
sro_embeddings = SentenceTransformer('jhgan/ko-sroberta-multitask')


shinsegae_gangnam = """ 
    ë§¤ì¥ëª…: ì‹ ì„¸ê³„ë°±í™”ì  ê°•ë‚¨ì , ì‹ ê°• í˜¹ì€ ì‹ ì„¸ê³„ ê°•ë‚¨ ì´ë¼ê³ ë„ ë¶€ë¦„
    ì „í™”ë²ˆí˜¸: 1588-1234
    ê¸ˆìš”ì¼,í† ìš”ì¼,ì¼ìš”ì¼ì€ ì—°ì¥ì˜ì—…. ì˜ì—…ì‹œê°„ì˜ ê²½ìš° ì§ˆë¬¸ ìš”ì¼ì„ ì°¸ê³ í•˜ì—¬ ë‹µë³€. ë§¤ì¥ë³„ ì‹œê°„ì´ ìˆëŠ”ê²½ìš° ë§¤ì¥ ìš´ì˜ì‹œê°„ì„ ì°¸ê³ .
    ì •ìƒ ì˜ì—… 10:30~20:00
    ì—°ì¥ ì˜ì—… 10:30~20:30
    íœ´ì ì¼: 2024ë…„ ê¸°ì¤€ 11ì›”27ì¼, 12ì›”ì€ íœ´ì ì¼ ì—†ìŒ
    ì§€í•˜1ì¸µ í‘¸ë“œì½”íŠ¸ëŠ” ë°±í™”ì  ì˜ì—…ì‹œê°„ê³¼ ë™ì¼í•¨
    11ì¸µ ì „ë¬¸ ì‹ë‹¹ê°€ì˜ì—… 11:00~21:30 ì‹ë‹¹ê°€ ì£¼ë¬¸ ë§ˆê° íì  30ë¶„ ì „(ë‹¨, ì¬ë£Œ ì†Œì§„ ì‹œ ì¡°ê¸° ë§ˆê° ê°€ëŠ¥)
    [í•˜ìš°ìŠ¤ ì˜¤ë¸Œ ì‹ ì„¸ê³„] ì£¼ì¤‘/ì£¼ë§ 10:30~22:00 * ì£¼ë¬¸ ë§ˆê° 1ì‹œê°„ ì „(ë‹¨, ì¬ë£Œ ì†Œì§„ ì‹œ ì¡°ê¸° ë§ˆê° ê°€ëŠ¥)
    [B1 ê¹Œì‚¬ë¹ ë³´/ë Œìœ„ì¹˜/ë¸Œë¼ìš°í„°/ì»¤í”¼ìŠ¤ë‹ˆí¼/íŒŒì´ë¸Œê°€ì´ì¦ˆ] 10:30-21:00 (ì—°ì¥ ì˜ì—… 10:30-21:30)
    [ì‹ ì„¸ê³„ íŒ©í† ë¦¬ìŠ¤í† ì–´ ê°•ë‚¨ì ] ì£¼ì¤‘/ì£¼ë§ 11:00~21:00
"""

def get_data_from_vector_db(query):
    query_embedding = sro_embeddings.encode(query).tolist()

    # Pineconeì—ì„œ ìœ ì‚¬í•œ í•­ëª© ê²€ìƒ‰
    results = pc_i.query(
        vector=query_embedding,
        top_k=20,
        include_metadata=True
    )

    store_text = ''
    # ê²€ìƒ‰ ê²°ê³¼ ì¶œë ¥
    for match in results['matches']:
        print(f"Score: {match['score']}, Name: {match['metadata']['name']}, Category: {match['metadata']['category']}, Floor: {match['metadata']['floor']}, Info: {match['metadata']['info']}")
        store_text += f"ë§¤ì¥ëª…: {match['metadata']['name']}, Category: {match['metadata']['category']}, Floor: {match['metadata']['floor']} Infomation: {match['metadata']['info']}\n"
    
    return store_text


# Create an OpenAI client.
# client = OpenAI(api_key=OPENAI_API_KEY)

# Create a session state variable to store the chat messages. This ensures that the
# messages persist across reruns.
if "messages" not in st.session_state:
    st.session_state.messages = []

# Display the existing chat messages via `st.chat_message`.
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Create a chat input field to allow the user to enter a message. This will display
# automatically at the bottom of the page.
if prompt := st.chat_input("What is up?"):

    # Store and display the current prompt.
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Generate a response using the OpenAI API.
    # stream = client.chat.completions.create(
    #     model="gpt-4o-mini",
    #     messages=[
    #         {"role": m["role"], "content": m["content"]}
    #         for m in st.session_state.messages
    #     ],
    #     stream=True,
    # )

    store_text = get_data_from_vector_db(prompt)
    context = shinsegae_gangnam
    today = datetime.now().strftime("%Y-%m-%d (%A)")
    location = "ì‹ ì„¸ê³„ ê°•ë‚¨ì "
    enhanced_prompt = f"""
        ì•„ë˜ ì •ë³´ë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ë‹µë³€ì„ í•©ë‹ˆë‹¤. ì§€ê¸ˆ ì§ˆë¬¸í•˜ëŠ” ì‚¬ëŒì€ í•´ë‹¹ ë°±í™”ì  ì•ˆì— ìˆìŠµë‹ˆë‹¤.:
        ì˜¤ëŠ˜: {today}
        ìœ„ì¹˜: {location}
        ë°±í™”ì  ì •ë³´: {context}
        ì§ˆë¬¸ ê´€ë ¨ ë§¤ì¥ ì •ë³´: {store_text}
        ì§ˆë¬¸: {prompt}
        ë‹µë³€:
    """
    stream = llm(enhanced_prompt).content

    # Stream the response to the chat using `st.write_stream`, then store it in 
    # session state.
    with st.chat_message("assistant"):
        response = st.write(stream)
    st.session_state.messages.append({"role": "assistant", "content": response})
